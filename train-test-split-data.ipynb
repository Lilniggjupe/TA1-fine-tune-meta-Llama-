{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e224efc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kentdry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kentdry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3050 6GB Laptop GPU. Num GPUs = 1. Max memory: 6.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"  # <- ini penting\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "model_path = r\"C:\\Users\\Kentdry\\Documents\\VSCODE\\TA1(Deepseek)\\models--unsloth--llama-3.2-1b-instruct-unsloth-bnb-4bit\\snapshots\\0a4436e20494a6504464ce35274b7e53fb7883d0\"  # lengkapin path-nya\n",
    "max_seq_length = 2048  # Maximum number of tokens processed at once\n",
    "dtype = None  # Default data type (adjusts automatically)\n",
    "load_in_4bit = True  # Enable 4-bit quantization to save memory\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_path,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "989de7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Gabungan_data_TA1.csv\", na_values=[\"#N/A\", \"#n/a\", \"n/a\", \"NA\", \"na\"])\n",
    "important_columns = [\n",
    "    \"sem_01_GPA\", \"sem_02_GPA\", \"sem_03_GPA\",\n",
    "    \"sem_01_CGPA\", \"sem_02_CGPA\", \"sem_03_CGPA\",\n",
    "    \"application_count\", \"major_name_opcs\", \n",
    "    \"gender\"\n",
    "]\n",
    "df_cleaned = df.dropna(subset=important_columns)\n",
    "df_cleaned = df_cleaned.drop(columns=[\"NIM\", \"sem_01_CGPA\",\"sem_02_CGPA\",\"form_number\",\"application_count\",\"school_geo_unit\",\"sem_01_GPA\", \"sem_02_GPA\", \"sem_03_GPA\"])\n",
    "dataset_ds = Dataset.from_pandas(df_cleaned)\n",
    "\n",
    "\n",
    "split_dataset = dataset_ds.train_test_split(test_size=0.05, seed=42)\n",
    "dataset_ds_train = split_dataset[\"train\"]\n",
    "dataset_ds_test = split_dataset[\"test\"]\n",
    "\n",
    "dataset_ds_train = dataset_ds_train.to_pandas()\n",
    "dataset_ds_train.to_csv(\"dataset_ds_train.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1847d474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging columns: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 118/118 [00:00<00:00, 10699.29 examples/s]\n",
      "Converting to ShareGPT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 118/118 [00:00<00:00, 39261.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth import to_sharegpt\n",
    "dataset = to_sharegpt(\n",
    "    dataset_ds_test ,\n",
    "    merged_prompt = (\n",
    "        \"The following are the student High School grade and student data.\"\n",
    "        \"[[The student scored {ENG} in English, and {MATH} in Math.]]\"\n",
    "        \"[[ They also scored {BIO} in Biology,{CHEM} in Chemistry, and {PHY} in Physics.]]\"\n",
    "        \"[[ They also scored {ECON} in Economics,{GEO} in Geography, and {SOC} in Social.]]\"\n",
    "        \"[[ They scored {FINAL} on their final year of high school exam.]]\"\n",
    "        \"[[ They applied {major_name_opcs} as a major.]]\"\n",
    "        \"[[ Their father's occupation is {father_occupation} and mother's occupation is {mother_occupation}.]]\"\n",
    "        \"[[ The student is {gender} and studied at {school_name} in {school_state}.]]\"\n",
    "        \"[[ The student takes the {curriculum_name} curriculum and is classified as {school_prop} school.]]\"\n",
    "    ),\n",
    "    conversation_extension = 1,\n",
    "    output_column_name = \"sem_03_CGPA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08099e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_to_alpaca_format(dataset):\n",
    "    alpaca_data = []\n",
    "    conversations = dataset['conversations']\n",
    "    instruction = \"Based on the data given, predict their Cumulative GPA for the third semester\"\n",
    "\n",
    "    for i in range(len(conversations)):\n",
    "        if conversations[i][0]['from'] == 'human' and conversations[i][1]['from'] == 'gpt':\n",
    "            alpaca_data.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": conversations[i][0]['value'],\n",
    "                \"output\": \"their third semester cumulative GPA is \" + conversations[i][1]['value']\n",
    "            })\n",
    "\n",
    "    return alpaca_data\n",
    "\n",
    "alpaca_dataset = convert_all_to_alpaca_format(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4264c05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|eot_id|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to format the dataset to fit our prompt training style \n",
    "EOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which the model when to stop generating text during training\n",
    "EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d30144be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 118/118 [00:00<00:00, 23607.34 examples/s]\n",
      "Creating json from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 475.28ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "186282"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_templates=\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question \n",
    "    ### Instruction:\n",
    "    {}\n",
    "    ### Input:\n",
    "    {}\n",
    "    ### Response:\n",
    "    {}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    instructions = example[\"instruction\"]\n",
    "    inputs       = example[\"input\"]\n",
    "    outputs      = example[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = chat_templates.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import Dataset\n",
    "# Konversi list of dict ke HuggingFace Dataset\n",
    "alpaca_dataset = Dataset.from_list(alpaca_dataset)\n",
    "alpaca_dataset = alpaca_dataset.map(formatting_prompts_func, batched = True)\n",
    "\n",
    "alpaca_dataset.to_json(\"dataset_ds_test.json\", orient=\"records\", lines=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
